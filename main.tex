\documentclass[preprint1]{aastex}
\usepackage[printonlyused]{acronym}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{verbatim}
\usepackage{mathtools}
\usepackage{float}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{url}
\usepackage{solarized-light}
\usepackage[caption=false]{subfig}
\usepackage{rotating}

\newcommand\pcp{\mbox{ pc}^{-3}}
\newcommand\Msun{\; M_\odot}
\newcommand\Myr{\mbox{ Myr}}
\newcommand\Gyr{\mbox{ Gyr}}
\newcommand\pc{\mbox{ pc}}
\newcommand\gram{\mbox{ g}}
\newcommand\nbody{\texttt{NBody6++GPU }}


\numberwithin{equation}{section}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
\graphicspath{{graphics/}}

\title{}
\author{Elias Rubin}
\begin{document}
% running out of time because black hole mass is the seed mass 
% at redshift above 7 we already see MBh around a billion solar masses
% need half Gyr tops.  So M seed must be ~10^4-10^5
% Lbh, quasar is some constant times black hole mass
% hence there is an observational constraint
\section{Scientific Motivation} \label{Intro}
Massive stars are believed to form the seed masses for \acp{IMBH} and \acp{SMBH} \citep{2003Bromm, 2013Hosokawa}. Observations of high redshift ($z \ge 6$) quasars place a constraint on the time allowed for \acp{IMBH} and \acp{SMBH} to form.  To explain \acp{SMBH} with masses over $10^9 \Msun$ forming under $1 \Gyr$, we require massive stars with masses around $10^4 - 10^5 \Msun$ to form within a few $\Myr$. These massive stars would then collapse due to general relativistic instability \citep{1964Chandrasekhar} into black holes which continue to accrete mass from their surroundings at the Eddington rate, as well as by continued mergers \citep{2015Katz, 2016Latif}. These \acp{IMBH} and \acp{SMBH} could result in powering many of the bright quasars we observe at high redshift.

In this work we examine the possibility of massive star formation by repeated collision in globular clusters and ultracompact dwarf galaxies. Sufficiently dense clusters can be highly collisional, and provide for the possibility of runaway growth. As a single star continues to increase in mass and stellar radius, it increases the likelihood of further collisions, and provides an avenue for runaway star growth \citep{2015Katz}. This mechanism for black hole formation is one of three that are commonly considered, the other two being core collapse black holes which typically are around $10 \Msun$, and the direct collapse of gaseous clouds, as described in \citet{2003Bromm} and \citet{2016Latif}. We follow the work of \citet{2004SPZ} and others by performing direct N-Body simulations of dense star clusters.

Core collapse black holes are believed to be too small to have much infuence on quasar abundance, as the seed mass is insufficient to form a $10^9 \Msun$ black hole at redshifts above $z = 7$.  Although direct collapse black holes have been investigated as a promising path to \acp{SMBH} formation, \citet{2016Latif} and others raise questions about the viability of that pathway because of significant radiation feedback effects which can reduce the rate of mass accretion below the $0.1 \Msun/\mbox{yr}$ required to develop sufficiently large seed mass black holes.

A time constraint on the formation of our seed mass is the main-sequence lifetime of the most massive stars in the cluster. For a typical initial mass function with a maximum mass of $\sim100 \Msun$, this time is about $3 \Myr$, afterwards stellar evolution effects such as supernovae and stellar winds can be expected to prevent runaway \citep{2002SPZ, 2016Latif}.  We also expect that the largest stars initially dominate the set of stars that do ultimately form runaways, again based on their larger mass and radius increasing the likelihood of collision.

\citet{2002SPZ} suggests runaway collisions as a pathway to \ac{IMBH} formation, with runaway masses at approximately $0.1\%$ of the total cluster mass in the clusters simulated.  \citet{2004SPZ} 

\textbf{Should observational constraints or other literature be discussed in this section?  What about more detail about alternative \ac{SMBH} pathways like in Latif?}

\textbf{Cite for relationship between quasar luminosity and black hole mass?}

\section{Model} \label{Model}
\textbf{Questions to answer:
What are the observed bodies that influence model?  May need some sources for this.
Put in a table/grid of all parameters surveyed.}
In their investigation of collision clusters MGG-9 and MGG-11, \citet{2004SPZ} identify two factors about collisional clusters that can lead to supermassive star formation: concentration parameter $w$ and dynamical time $t_{\mathrm{df}}$, defined as follows:

\begin{subequations}
    \begin{align*}
    w &= \log{\frac{R_t}{R_c}} \\
    t_{\mathrm{df}} &= \frac{\langle m \rangle}{100 \Msun} \frac{0.138N}{\ln{0.11M/100\Msun}}\left(\frac{R^3}{GM}\right)^{1/2}
    \end{align*}
\end{subequations} 

\textbf{Should put definitions of each parameter?}

\textbf{}


\section{Simulation Methods} \label{Methods}

\textbf{Does anything else belong in this section?  A discussion of units, important cluster properties, more detail on numerical methods?}
We are interested in following the evolution of collisional clusters and exploring the range of parameters in which they can lead to \ac{SMBH} formation. Finding a solution entails solving the equation of motion for each particle specified by the force $\mathbf{F}_{i}$ and its time derivative $\mathbf{F}^{(1)}_{i}$ as follows:
\begin{subequations}
    \begin{align}
    \mathbf{F}_{i} &= -m_{i}\sum_{i \neq j} Gm_{j} \frac{\mathbf{R}}{R^3}; \\
    \mathbf{F}^{(1)}_{i} &=  -m_{i}\sum_{i \neq j} Gm_{j} \left[ \frac{\mathbf{V}}{R^3} + \frac{3\mathbf{R}(\mathbf{V} \cdot \mathbf{R})}{R^5}\right],
    \end{align}
    \label{eqn:gravmotion}
\end{subequations}
where $G$ is the gravitational constant, $m_j$ is the mass of particle $j$, and $\mathbf{R}$ and $\mathbf{V}$ are the vector distance and velocity respectively between any two particles $i$ and $j$.  Because it is not possible to analytically solve the dynamical equations for every body in a cluster, systems must be evolved numerically. Developing numerical methods for N-Body simulations is a long tradition, but the two most popular methods are tree-based simulations and direct simulations. The tradeoff between the two is one of computational cost versus accuracy. Notwithstanding implementation details, direct simulations compute pairwise force terms for every pair of bodies at each discrete simulation timestep, which comes with a computational cost per timestep of $O(N^2)$ in the number of particles.  Tree-based simulations partition the simulation space and compute pairwise terms for close neighbors of a given particle, but aggregate forces from groups outside of a close radius. Thus, the cost per timestep goes as $O(N\mathrm{log}N)$.

The metric generally used for simulation error is energy conservation across the system. Practicioners focus their efforts on increasing simulation speed while keeping increases to errors within acceptable bounds. The acceptable level of error depends on the problem domain and desired fidelity of the result.  For our cluster simulations we have many bodies in close proximity and care about the accurate treatment of close encounters, binaries, multiple systems, and collisions, so we use \nbody, a direct code.  An alternative we considered is the tree-based code \texttt{Starlab}, but our initial testing and recommendations from practicioners suggested that \nbody would be better for our use case.

We perform direct N-body simulations over a range of cluster parameters varying in total mass, half-mass radius, and concentration.  We use the \texttt{McLuster} software of \citet{2011Kupper} to generate initial conditions and the \nbody software of \citet{2015Wang} to evolve the clusters. We also describe some modifications to the \nbody software made in attempts to accelerate simulations.

\textbf{What is the integrator scheme; how does it work?  What are the numerical methods used?}
\nbody is the latest version in a family of direct N-body simulators beginning with \texttt{NBody1} of \citet{1963Aarseth}. \citet{1999Aarseth} describes in detail the evolution of the \texttt{NBody} family of programs and numerical methods therein.  The code is written primarily in Fortran but has modules written in CUDA for GPU extensions and C++ for access to AVX/SSE instructions. It is parallelized with OpenMP.

In order to solve equations~\ref{eqn:gravmotion}, the \nbody integrator uses a direct fourth-order Hermite integration method, as well as a hierarchical block step \citep[][and refs within]{2015Wang}. This is a kind of adaptive timestep system. Instead of evaluating all particles at every timestep, particles are binned into quantized groups, and only those particles at an integer multiple of their timestep are integrated. This speeds up the integration of slow moving particles, while still allowing for accuracy for more extreme particles. The code uses the neighbor scheme of \citet{1973Ahmad} to speed up the integration further by separating into the more economical regular force (forces on a body generated by bodies outside a neighbor radius) and irregular force (forces generated by bodies within a neighbor radius). In \nbody, the regular force computations are done on the GPU, and irregular force computations are done on the CPU with the AVX/SSE library of \citet{2012Nitadori}. The next part of this section explains each of these in more detail.

The Hermite integration method is used to solve equations~\ref{eqn:gravmotion}.  This is a predictor-corrector method that uses a third-order Taylor expansion.  The expression for the force is described in \citet{1999Aarseth} and is as follows:

\begin{subequations}
\begin{align}
    \mathbf{F}_{t+1} &= \mathbf{F}_{t} + \mathbf{F}^{1}_{t}\Delta t + \frac{1}{2}\mathbf{F}^{(2)}_{t}\Delta t^2 + \frac{1}{6}\mathbf{F}^{(3)}_{t}\Delta t^3, \\
    \mathbf{F}^{(1)}_{t+1} &= \mathbf{F}^{(1)}_{t} + \mathbf{F}^{(2)}_{t}\Delta t + \frac{1}{2}\mathbf{F}^{(3)}_{t} \Delta t^2.
\end{align}
\label{eqn:LowOrderHermite}
\end{subequations}
$\mathbf{F}$ and $\mathbf{F}^{(1)}$ can be computed at the beginning and end of a timestep by using equations~\ref{eqn:gravmotion}. They are used to form the higher order terms
\begin{subequations}
\begin{align}
    \mathbf{F}^{(2)}_{t} &= \frac{2}{\Delta t^2}\left[-3(\mathbf{F}_{t} -\mathbf{F}_{t+1}) - 2(\mathbf{F}^{(1)}_{t} + \mathbf{F}^{(1)}_{t+1}) \Delta t\right], \\
    \mathbf{F}^{(3)}_{t} &= \frac{6}{\Delta t^3}\left[2(\mathbf{F}_{t} - \mathbf{F}_{t+1}) + (\mathbf{F}^{(1)}_{t} + \mathbf{F}^{(1)}_{t+1})\Delta t\right].
\end{align}
\label{eqn:HigherOrderHermite}
\end{subequations}
Then the predictor and corrector $\mathbf{r}_{p, t+1}, \mathbf{v}_{p, t+1}, \Delta \mathbf{r}$, and $\Delta \mathbf{v}$ are given thusly
\begin{subequations}
\begin{align}
    \mathbf{r}_{p, t+1} &= \mathbf{r}_{t} + \mathbf{v}_{t} \Delta t + \frac{1}{2} \mathbf{F}_{t} \Delta t^2 + \frac{1}{6} \mathbf{F}^{(1)}_{t} \Delta t^3, \\
    \mathbf{v}_{p, t+1} &= \mathbf{v}_{t} + \mathbf{F} \Delta t + \frac{1}{2}\mathbf{F}^{(1)}_{t} \Delta t^2, \\
    \Delta \mathbf{r} &= \frac{1}{24} \mathbf{F}^{(2)}_{t} \Delta t^4 + \frac{1}{120}\mathbf{F}^{(3)}_{t} \Delta t^5, \\
    \Delta \mathbf{v} &= \frac{1}{6} \mathbf{F}^{(2)}_{t} \Delta t^3 + \frac{1}{24}\mathbf{F}^{(3)}_{t} \Delta t^4,
    \end{align}
\label{eqn:PredCorr}
\end{subequations}
with the final state at $t+1$ given by $\mathbf{r}_{t+1} = \mathbf{r}_{p, t+1} + \Delta \mathbf{r}$ and $ \mathbf{v}_{t+1} = \mathbf{v}_{p, t+1} + \Delta \mathbf{v}$. 

The Hermite predictor is used for each time step in the hierarchical block step scheme. Block steps are the way the \texttt{NBODY} family of codes addresses the large dynamic range in the velocities and proximities of bodies in clusters. This is necessary for performance because otherwise the time step for the entire system would be determined by the separation of the closest bodies. Instead each particle is given its own timestep using the following formula of \citet{2017Khalisi} 

\begin{equation}
\Delta t_{i} = \sqrt{\eta \frac{\lvert \mathbf{a}_{1,i}\rvert \lvert \mathbf{a}^{(2)}_{1,i}\rvert + \lvert \mathbf{a}^{(1)}_{1,i} \rvert ^2}{\lvert \mathbf{a}^{(1)}_{1,i} \rvert \lvert \mathbf{a}^{(3)}_{i,1} \rvert \lvert \mathbf{a}^{(2)}_{1,i}\rvert^2}}.
\label{eqn:blocksteptime}
\end{equation}

We use an initial value of $0.01$ for $\eta$ which we arrived at as it generally allowed our simulations to progress further before energy error becomes too great which forces a restart.  This value may be further adjusted if the relative energy error $dE$ (defined as the difference in total simulation energy between checkpoint times) becomes close (within a factor of 5) to the maximum tolerance $Q$. The correction factor is given by $\sqrt{dE/Q}$.  Our simulations use a checkpoint timestep of $0.01$ $t_{\mathrm{nb}}$, where $t_{\mathrm{nb}}$ is the simulation time unit and is equal to $\frac{1}{2\sqrt{2}} t_{\mathrm{df}}$ \citep{2017Khalisi}. We set $Q=0.05$, which allows for a maximum energy error of $20\%$, and causes a readjustment of $\eta$ when the energy error reaches $4\%$.

Figure~\ref{fig:blockstep} shows an example of how the particles are advanced. 
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{KhalisiTimeStep}
    \caption{Particles $i$, $k$, $l$, and $m$ have separate time steps. A full force computation is done at each arrow, otherwise, the position and velocity are extrapolated from the values already computed.  These time steps are reevaluated by equation~\ref{eqn:blocksteptime} after a full integration cycle. Subsystems which are evaluated independently are replaced by their center of mass in the block scheme. Figure reproduced from \citet{2017Khalisi}.}
    \label{fig:blockstep}
\end{figure}

The \nbody employs the neighbor scheme of \citet{1973Ahmad} in conjunction with the Hermite integrator and block timestep. This is a further optimization to speed up the calculations, this time by identify particles that are spatially close to each other. Thus, a given particle will actually have two timesteps, a regular timestep and (smaller) irregular timestep. Particles have a neighbor list of size at most $N_{\mathrm{nb}} \ll N_{\mathrm{tot}}$.  These are advanced using a similar predictor-corrector as the Hermite scheme above, computing the full force for neighbor particles and the last regular values for non-neighbors. Membership in the neighbor list is determined by the size of the neighbor sphere and the length of the neighbor list $N_{\mathrm{nb}}$.  The first $N_{\mathrm{nb}}$ particles within a sphere of radius $r_{\mathrm{search}}$ are included.  We found that too small values for $N_{\mathrm{nb}}$ would lead to code crashes and settled on a value of 1024.  We use a search sphere of radius $1 \times 10^{-4} \pc$.

The distinguishing feature of the \texttt{NBODY} family is their use of \citet{1965Kusta} (KS) regularization to generate solutions for binaries, triples, and multiple systems with high levels of accuracy.  This treatment for close encounters adds a good deal of complexity to the implementation, but the scheme essentially works by searching for bodies closer to each other than $r_{\mathrm{min}}$, replacing them in the next integration step by their center of mass, and integrating them separately at a smaller timescale and in a different reference frame. This is useful to avoid numerical truncation errors caused by bodies being too close in the originally reference frame. The KS regularization is extended to multiple systems and adds perturbing bodies using a variant of the \citet{1973Ahmad} neighbor scheme. KS pairs and multiples are terminated if the distance between the bodies exceeds $r_{\mathrm{min}}$ or if the bodies collide, in which case they are merged.  We discuss the merger criteria and method later in this section.

Although \nbody uses MPI parallelization to scale across multiple compute nodes, there are some sequential bottlenecks including KS regularization. In our experiments, we experienced no gain from using multiple nodes, and it's possible that the KS regularization method was responsible. We touch on this further in section~\ref{conclusions}.

We performed our simulations on the \texttt{Tiger2} GPU cluster at Princeton University. Most experiments were performed on one compute node which each use four NVIDIA Tesla P100 GPUs and 2.4 GHz Broadwell processors parallelized with OpenMP.

\textbf{What are the initial conditions and how are they generated?}
We generate our initial conditions using the \texttt{McLuster} software of \citet{2011Kupper}. For the initial distribution of positions and velocities, we use a \citet{1966King} profile, varying half-mass radius and concentration.  We use the initial mass function of \citet{2001Kroupa} which degenerates to a Saltpeter IMF above $0.5 \Msun$.  For our simulations with cluster mass $2 \times 10^5 \Msun$, we use a mass range of $1$ to $100 \Msun$.  For cluster mass $6 \times 10^5 \Msun$, we increase the lower bound to $3.3 \Msun$.  For cluster mass $2 \times 10^6 \Msun$, we use a lower bound of $13 \Msun$ and an upper bound of $120 \Msun$.  We do not apply an external tidal field, initialize primordial binaries, or apply primordial mass segregation. 

\textbf{What are the boundary and escape conditions?}
We use an open boundary condition so particles that do escape do not return to the simulation. Our escape condition is a distance from center of greater than twice the tidal radius. Both of these conditions remain uniform over all of our simulations.

\textbf{What is the merger scheme?}
The default merger criteria in \nbody follows that of \citet{1992Kochanek}.  For a two-body system with pericenter $R_p$, the bodies are merged if

\begin{equation}
 \frac{R_{p}}{R_{1}} \leq 1.3 \times (\frac{M_T}{2M_1})^{1/3}.
 \label{eqn:kochanekcriteria}
\end{equation}

This merger scheme is conservative.  In the case of a central mass with $m = 200 \Msun$ (a typical case after a couple of mergers) and an additional mass with $m = 1 \Msun$, and using $r_{\mathrm{central}} = (\frac{m}{\Msun})^{0.55}$, we would not merge the two stars unless they had a pericenter distance smaller than $20$ stellar radii, or a deviation of only about $3\%$ from the radius of the central mass.

In an effort to increase the speed of our simulations by spending less time in KS regularization for insignificant bodies, we experimented with short-circuiting this merger scheme for pairs meeting certain conditions.  We experimented with merging binaries where one member was $5$ or $10$ times larger than the other, provided the pericenter distance was smaller than $10 r_\mathrm{large}$ (in the more conservative trial) and $100 r_\mathrm{large}$ in the more liberal one.  We avoided merging with the central mass, but did want to reduce the number of KS pairs for peripheral bodies. We found that the overall runtime was not affected significantly by this change to the merger scheme. Simulation results appeared robust to the factor $10$ difference, but were affected by the factor $100$ difference.  We tested changes to our merger scheme at approximately the transition between collisional and non-collisional clusters: a mass of $2 \times 10^5 \Msun$, a concentration of $2$, and a half-mass radius of $2.9 \pc$.

\textbf{How was verification performed?}
\citet{2009Anders, 2012Anders} perform comparisons between \texttt{NBody4} of \citet{1999Aarseth} and \texttt{Starlab} environment.  In order to verify our installation of \nbody we repeated the tests from \citet{2009Anders}.  We used \texttt{McLuster} to generate similar initial conditions to the ones used in their test. Our verification clusters consist of 1024 $1 \Msun$ stars, distributed in a Plummer profile with initial half-mass radius $0.5 \pc$.  We performed test runs with and without stellar evolution mass loss, as \nbody requires stellar evolution mass loss to perform mergers.

The trends of our verification runs generally agreed with those in \citet{2009Anders}.  The authors left some uncertainty about the exact parameters used for their runs, and they also sampled many runs to obtain an average. Figures~\ref{fig:verifications1} and ~\ref{fig:verifications2} compare the two.

\begin{figure}%
    \centering
     \subfloat[Half-mass radius vs. time reproduced from \citet{2009Anders}.]{{\includegraphics[width = 7cm]{AndersHalfRadius}}}%
    \qquad   
    \subfloat[Core radius vs. time reproduced from \citet{2009Anders}.]{{\includegraphics[width = 7cm]{AndersCoreRadius}}}%
    \qquad
    \subfloat[Half-mass radius vs. time from our verification run.]{{\includegraphics[width = 7cm]{VerifyHalfRadius}}}%
    \qquad
    \subfloat[Core radius vs. time from our verification run.]{{\includegraphics[width = 7cm]{VerifyCoreRadius}}}%
    \caption{Our verification run shows generally good agreement with those of \citet{2009Anders}.  The half mass radius doesn't start to expand significantly until $200$ time steps in both sets, and then follows a general upwards trend.  Our core radius shows a similar peak-trough distance of oscillation and a return close to the starting point by the end of the run.
    Our result appears to oscillate more because we show a single run instead of an entire sample. }
    \label{fig:verifications1}
\end{figure}

\begin{figure}%
    \centering
     \subfloat[Kinetic energy vs. time reproduced from \citet{2009Anders}.]{{\includegraphics[width = 7cm]{AndersKinetic}}}%
    \qquad   
    \subfloat[Potential energy vs. time reproduced from \citet{2009Anders}.]{{\includegraphics[width = 7cm]{AndersPotential}}}%
    \qquad
    \subfloat[Kinetic energy vs. time from our verification run.]{{\includegraphics[width = 7cm]{VerifyKinetic}}}%
    \qquad
    \subfloat[Potential energy vs. time from our verification run.]{{\includegraphics[width = 7cm]{VerifyPotential}}}%
    \caption{We see the same general trend with kinetic and potential energy from our verification run, although the kinetic energy from our run seems to end up lower than that of \citet{2009Anders}.  This is where there is also the greatest divergence between \texttt{NBODY4} and \texttt{Starlab}, so it's possible that changes in between \texttt{NBODY4} and \nbody account for this difference.}
    \label{fig:verifications2}
\end{figure}


\textbf{What other issues with original code?  How were they resolved?}
We had significant difficulty running more collisional clusters to completion.  We found that clusters with many collisions stretched the limits of default \nbody parameters.  Among other issues, we encountered a persistent problem where runs would stop progressing during a KS regularization.  We settled on a two-part workaround for this problem: first by reducing the KS step size which seemed to forestall these failures at the cost of some additional compute time and second by simply restarting runs once they stopped progressing. 

We were unable to get \nbody's internal restart module to function in our environment, so our restart method required building a tool to parse the checkpoint data and create initial conditions for a fresh simulation.  The raw checkpoint data does not perfectly reflect the state of the system because it contains duplicate bodies when binaries, triples, quads, and chains are involved. We implemented an additional heuristic in our restart code to remove spurious binaries and other multiples, leaving only the constituent bodies in the new initial conditions.

\nbody's default configuration does not always seem to handle collisional clusters well. This led to other failure modes for our simulations which we tried to correct. In dense clusters we found that the neighbor lists would rapidly become full leading to code crashes.  Here we tried reducing the size of neighbor search spheres per the suggestion of \citet[][personal communication]{2017Wang}, which did not resolve the issue, so we instead doubled the size of the neighbor lists which was enough to alleviate the issue. Another failure mode which we have not yet been able to find a solution to occurs after restarts during GPU force initialization. It appears some initial conditions cause GPU out-of-memory errors which cause \nbody to crash. The resolution we have adopted for that failure mode is to binary search snapshots until finding one that proceeds and begin the run at that point.  We then have to invalidate data after the selected snapshot.

\section{Results} \label{Results}

\begin{sidewaysfigure}[ht]
    \centering
    \includegraphics{JobProgress}
    \caption{Not all jobs ran to completion.}
    \label{fig:JobProgress}
\end{sidewaysfigure}

\begin{figure}
    \includegraphics[width=20cm]{kplot}
    \caption{Plot of clusters with their parameter $K$}
    \label{fig:KPlot}
\end{figure}
\textbf{What clusters had runaway central masses?}

\textbf{What fraction of the total cluster mass did the runaway achieve?  How long didit take?}

\textbf{What was the rate of growth?  Did this vary with cluster parameters?  What about the shape of the growth curves?}

\textbf{Any clusters that were too expensive to simulate or didn't work for some other reason?}

\textbf{What other observed phenomena?  Did any/all clusters experience mass loss?  What happened with core radius?  Was core collapse correlated with larger central mass?}



\bibliographystyle{apj}
\bibliography{ref}
\acrodef{SMBH}{supermassive black hole}
\acrodef{IMBH}{intermediate-mass black hole}
\end{document}